Sooo 14:41:22 today's topic: Neural Networks ðŸ¤¯.
        Perceptrons, activation functions (ReLU, sigmoid, tanh), feed-forward networks.
        Backpropagation updates weights using gradient desc, basically.
        Issues like overfitting, underfitting, dropout, regularizationâ€“â€“ aaahhhh lots of terms ðŸ˜‚.
        Applications: images, speech, NLP, etc.
        Deep models need huge data + GPUs sooo yeah get ready ðŸ’€.