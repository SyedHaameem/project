{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec291c8c",
   "metadata": {},
   "source": [
    "libraries used ...\n",
    "\n",
    "TRANSFORMERS-> Gives Use All AI Models T5 , BART,Pegasus, GPT-2\n",
    "\n",
    "DATASETS ‚Üí To load and prepare text data ,This helps to load large datasets like:Lecture transcripts,Text files,Summarization datasets (CNN/DailyMail)\n",
    "Without datasets, we can't preprocess or train on any dataset efficiently.\n",
    "\n",
    "SENTENCEPIECE ‚Üí Tokenizer backend for T5/Pegasus...Models like T5 and Pegasus require SentencePiece tokenization.\n",
    "Without sentencepiece, the tokenizer for these models will break.\n",
    "\n",
    "ACCELERATE ‚Üí To speed up training and use GPU..This library makes training:Faster , Easier , Automatically uses GPU if available\n",
    "Without accelerate, finetuning models becomes slow and complex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21670850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "#T5Tokenizer converts text ‚Üí numbers (tokens)\n",
    "#This is the actual T5 model used for tasks like: Summarization, Translation ,Question ,Answering\n",
    "#This is a configuration object that tells the Trainer HOW to train your model...To control, learning rate ,,batch size\n",
    "\n",
    "from datasets import Dataset\n",
    "#Dataset is a class from the HuggingFace datasets library.\n",
    "#hugging face provides the tool kit to build and train the ML models\n",
    "#It represents a clean, efficient, memory-optimized table of data used for machine learning.\n",
    "\n",
    "import torch\n",
    "#torch is the main Python library of PyTorch, which is a deep learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49472b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "True\n",
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "##torch.cuda.is_available() is a PyTorch function that tells you whether your computer has a GPU that PyTorch can use.\n",
    "\n",
    "print(torch.__version__)  \n",
    "print(torch.version.cuda) \n",
    "print(torch.cuda.is_available()) \n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55732b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"lecture\": [\n",
    "\n",
    "        # Machine Learning\n",
    "        \"\"\"So, uhh... welcome üòÖ to today's class, 12:08:11. Today we're talking about Machine Learning, okay?\n",
    "        ML is, aaah, basically a part of AI that lets computers *learn* from data.\n",
    "        We have supervised... unsupervised... and umm reinforcement learning.\n",
    "        Supervised uses labeled daaata, unsupervised finds hidden patterns without labels, sooo yeah.\n",
    "        RL works on rewards + penalties ü§ñ.\n",
    "        We'll also talk about accuracy, precision, recall, F1‚Äì‚Äì score, etc.\n",
    "        Applications? Spam detection, fraud detection, cars that drive themselves üöóüí® and so on.\n",
    "        Clean data is important... like VERY important.\n",
    "        Also preprocessing: removing missing values, scaling, normalization.. blah blah.\n",
    "        Future trends: deep learning, Generative AI, LLMs üòé.\"\"\",\n",
    "\n",
    "        # DBMS\n",
    "        \"\"\"Alright sooo 09:14:55 today‚Äôs topic is Database Management Systems (DBMS)...\n",
    "        aaah okay so DBMS helps in storing + retrieving data efficiently.\n",
    "        We have three levels: physical, logical and external‚Äì‚Äì yeah remember that.\n",
    "        Relational databases, primary keys, foreign keys... normalization (1NF, 2NF, 3NF) etc.\n",
    "        Also indexing! Transaction management!\n",
    "        ACID properties ‚Üí Atomicity, Consistency, Isolation, Durability (umm very important) üëç.\n",
    "        Concurrency control... locks... timestamps... sooo users don‚Äôt mess each other up.\n",
    "        SQL vs NoSQL differences, you know.\n",
    "        Applications: banking üè¶, ecommerce, warehouses, etc.\"\"\",\n",
    "\n",
    "        # Operating Systems\n",
    "        \"\"\"Okay sooo, 10:03:07, today's lecture is on Operating Systems (OS).\n",
    "        OS acts as, umm... a bridge between user + hardware.\n",
    "        Responsibilities: process mgmt, memory mgmt, file mgmt, device control‚Ä¶ all that stuff.\n",
    "        SJF, FCFS, Priority, Round Robin scheduling‚Äì‚Äì remember these algorithms.\n",
    "        Memory? Paging + segmentation.\n",
    "        Deadlocks happen when processes wait forever üò≠.\n",
    "        Prevention, detection, avoidance... security features (auth, access control, encryption).\n",
    "        Examples: Windows, MacOS, Linux üêß.\"\"\",\n",
    "\n",
    "        #Neural Networks\n",
    "        \"\"\"Sooo 14:41:22 today's topic: Neural Networks ü§Ø.\n",
    "        Perceptrons, activation functions (ReLU, sigmoid, tanh), feed-forward networks.\n",
    "        Backpropagation updates weights using gradient desc, basically.\n",
    "        Issues like overfitting, underfitting, dropout, regularization‚Äì‚Äì aaahhhh lots of terms üòÇ.\n",
    "        Applications: images, speech, NLP, etc.\n",
    "        Deep models need huge data + GPUs sooo yeah get ready üíÄ.\"\"\",\n",
    "\n",
    "        #  Cloud Computing\n",
    "        \"\"\"Okay, quick lecture 11:55:10 on Cloud Computing ‚òÅÔ∏è.\n",
    "        Cloud = on-demand servers, storage, apps on internet, like AWS, GCP, Azure etc.\n",
    "        Service models: IaaS, PaaS, SaaS (remember pls).\n",
    "        Deployments: public, private, hybrid, community.\n",
    "        Characteristics: scalability, elasticity, pay-as-you-go sooo $$$ saved.\n",
    "        Virtualization + containers (Docker, Kubernetes üê≥).\n",
    "        Security challenges like data breaches üò¨, identity mgmt, compliance issues.\n",
    "        Used in streaming apps, e-commerce, enterprise IT.\"\"\",\n",
    "\n",
    "        # Data Science\n",
    "        \"\"\"Umm‚Ä¶ so 08:20:03 today's lecture is Data Science ü§ì.\n",
    "        Data science mixes statistics, programming, domain knowledge...\n",
    "        Steps: data collection ‚Üí cleaning ‚Üí exploration ‚Üí visualization ‚Üí modeling ‚Üí deployment.\n",
    "        Tools: Python, R, Pandas, NumPy, Matplotlib.\n",
    "        ML techniques like regression, clustering, classification.\n",
    "        Importance of feature engineering!!\n",
    "        Metrics: RMSE, R¬≤, precision, recall.\n",
    "        Applications: health, finance, marketing.\n",
    "        okay that‚Äôs it üòÇ.\"\"\",\n",
    "\n",
    "        # Computer Architecture\n",
    "        \"\"\"Sooo okay 16:18:40 today's Computer Architecture lecture.\n",
    "        It defines how hardware + software interact to run instructions.\n",
    "        It has ISA, microarchitecture, system architecture.\n",
    "        ISA ‚Üí instructions, registers, addressing modes.\n",
    "        RISC (ARM, RISC-V) vs CISC (x86).\n",
    "        Microarchitecture = ALU, control unit, caching, pipeline, branch prediction, superscalar stuff.\n",
    "        Kinda complicated ü§¶‚Äç‚ôÇÔ∏è but important.\"\"\",\n",
    "\n",
    "        # Internet of Things (IoT)\n",
    "        \"\"\"Aaah okay so IoT, 12:33:19‚Ä¶\n",
    "        IoT = devices connected through internet sending/receiving data automatically.\n",
    "        Sensors + connectivity + cloud + smart software = real-time decisions.\n",
    "        Applications: smart homes, smart cities, healthcare, farming, transport, industries.\n",
    "        Examples: thermostats, wearables, smart cameras, driverless cars üöóü§ñ, irrigation systems.\n",
    "        IoT improves automation & efficiency sooo yeah.\"\"\",\n",
    "\n",
    "        #Computer Networks\n",
    "        \"\"\"So today at 09:02:55 we discussed Computer Networks.\n",
    "        Network = devices connected to share data + resources.\n",
    "        Types: LAN, MAN, WAN, PAN.\n",
    "        Internet = largest WAN ever üåê.\n",
    "        Uses TCP/IP, HTTP, FTP protocols.\n",
    "        Components: routers, switches, servers, clients, etc.\n",
    "        Applications: communication, file sharing, cloud, business, education‚Ä¶ sooo everything.\"\"\",\n",
    "\n",
    "        # Cybersecurity\n",
    "        \"\"\"Umm okay last lecture 15:29:14 on Cybersecurity üîê.\n",
    "        Cybersecurity protects systems from attacks, malware, exploits etc.\n",
    "        Concepts: encryption, hashing, firewalls, antivirus, intrusion detection, authentication.\n",
    "        Threats: phishing üò≠, ransomware, DDoS, password attacks, MITM attacks.\n",
    "        CIA triad ‚Üí Confidentiality, Integrity, Availability.\n",
    "        Best practices: strong passwords, MFA, updates, backups, network monitoring.\n",
    "        Cybersecurity is super important today sooo be careful online üëÄ.\"\"\"\n",
    "    ],\n",
    "\n",
    "    \"summary\": [\n",
    "\n",
    "        \"This lecture explains machine learning types, evaluation metrics, preprocessing, and applications such as fraud detection and autonomous systems.\",\n",
    "\n",
    "        \"This lecture covers DBMS architecture, normalization, indexing, ACID properties, and differences between SQL and NoSQL databases.\",\n",
    "\n",
    "        \"This lecture explains OS responsibilities, scheduling algorithms, memory management, deadlocks, and common operating systems.\",\n",
    "\n",
    "        \"This lecture discusses neural networks, activation functions, backpropagation, overfitting solutions, and deep learning applications.\",\n",
    "\n",
    "        \"This lecture introduces cloud models, deployments, virtualization, containerization, and cloud security challenges.\",\n",
    "\n",
    "        \"This lecture outlines the data science pipeline, tools, ML techniques, and evaluation metrics.\",\n",
    "\n",
    "        \"This lecture explains computer architecture layers, ISA, RISC vs CISC, and microarchitecture components.\",\n",
    "\n",
    "        \"This lecture introduces IoT devices, architecture, applications, and benefits such as automation.\",\n",
    "\n",
    "        \"This lecture explains computer networks, types, protocols, devices, and real-world applications.\",\n",
    "\n",
    "        \"This lecture covers cybersecurity concepts, CIA triad, attack types, best practices, and security mechanisms.\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec28d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing\n",
    "\n",
    "import re #(Regular Expressions)is a powerful pattern matching langugage ,used\n",
    "#for search,find and manipulate text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "\n",
    "#removing the time stamp\n",
    "\n",
    "    text=re.sub(r'\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\b',\" \",text)\n",
    "# Finding thr filler words\n",
    "    filler_words = [\n",
    "        r\"\\buh+\\b\", r\"\\bum+\\b\", r\"\\bokay\\b\", r\"\\bso+\\b\", r\"\\byou know\\b\"\n",
    "    ]\n",
    "\n",
    "    #deleting the filler words\n",
    "    for fw in filler_words:\n",
    "        text = re.sub(fw, \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "#removing the emojs,symbols ,\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?;:\\s]\", \" \", text)\n",
    "\n",
    "#converts the text into the lower case\n",
    "    text = text.lower()\n",
    "\n",
    "#remove the extra space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1220c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today we are going to discuss deep learning... deep learning is a subset of ml!\n"
     ]
    }
   ],
   "source": [
    "sample=sample = \"\"\"\n",
    "00:01 Sooo today we are uh going to discuss Deep Learning...\n",
    "00:05 Uhhhh deep learning is you know a subset of ML!\n",
    "\"\"\"\n",
    "\n",
    "print(clean_text(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c77378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lectures files saved\n",
      "So, uhh... welcome üòÖ to today's class, 12:08:11. Today we're talking about Machine Learning, okay?\n",
      "        ML is, aaah, basically a part of AI that lets computers *learn* from data.\n",
      "        We have supervised... unsupervised... and umm reinforcement learning.\n",
      "        Supervised uses labeled daaata, unsupervised finds hidden patterns without labels, sooo yeah.\n",
      "        RL works on rewards + penalties ü§ñ.\n",
      "        We'll also talk about accuracy, precision, recall, F1‚Äì‚Äì score, etc.\n",
      "        Applications? Spam detection, fraud detection, cars that drive themselves üöóüí® and so on.\n",
      "        Clean data is important... like VERY important.\n",
      "        Also preprocessing: removing missing values, scaling, normalization.. blah blah.\n",
      "        Future trends: deep learning, Generative AI, LLMs üòé.\n"
     ]
    }
   ],
   "source": [
    "#saving the lectures\n",
    "\n",
    "import os\n",
    "os.makedirs(\"content/lectures\", exist_ok=True)#make a folder  (in clob content is the n main working directory)\n",
    "\n",
    "for i ,text in enumerate(data[\"lecture\"],start=1):\n",
    "  #enumerate() fuction give index number{i} and the value{text}\n",
    "#create the file path\n",
    "  file_path =f\"content/lectures/lecture_{i}.txt\"\n",
    "  #write into the file\n",
    "  with open (file_path,\"w\", encoding=\"utf-8\")as f:#encoding=\"utf-8\" ensures all charaters present\n",
    "    f.write(text)\n",
    "\n",
    "print(\"lectures files saved\")\n",
    "path = \"content/lectures\"\n",
    "\n",
    "#sorting files numerically ,in side the /content/lectures ,with Rule {KEY}\n",
    "files = sorted(\n",
    "    os.listdir(path), key=lambda x: int(x.split(\"_\") #split at _ {\"lecture\",\"2.txt\"}\n",
    "       #take the numeric part of it of indix [1]\n",
    "    [1].split(\".\")[0]) #then split that thing at \".\" and take index [0]\n",
    "    )  #convert that string into the integer\n",
    "files\n",
    "with open (f\"content/lectures/lecture_{1}.txt\",\"r\") as f:\n",
    "  print(f.read())\n",
    "\n",
    "\n",
    "\n",
    "#os.listdir(\"/content/lectures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffb5fef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', ... welcome to today s class, . today we re talking about machine learning, ? ml is, aaah, basically a part of ai that lets computers learn from data. we have supervised... unsupervised... and reinforcement learning. supervised uses labeled daaata, unsupervised finds hidden patterns without labels, yeah. rl works on rewards penalties . we ll also talk about accuracy, precision, recall, f1 score, etc. applications? spam detection, fraud detection, cars that drive themselves and on. clean data is important... like very important. also preprocessing: removing missing values, scaling, normalization.. blah blah. future trends: deep learning, generative ai, llms .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#saving the clean lectures\n",
    "\n",
    "import glob #helps python to find .txt or .jpg fies\n",
    "cleaned_texts= []\n",
    "\n",
    "for file in glob.glob(\"content/lectures/*.txt\"): #finf all {.txt} files and open them\n",
    "  with open(file, \"r\", encoding= \"utf-8\") as f:\n",
    "    raw = f.read()\n",
    "    cleaned = clean_text(raw) #we  clean the raw content with predefined regex functiom{clean_text}\n",
    "    cleaned_texts.append(cleaned) #add the clean content in cleaned_texts\n",
    "\n",
    "#new folder for clean files\n",
    "os.makedirs(\"content/cleaned/\",exist_ok=True)\n",
    "\n",
    "for i ,text in enumerate(cleaned_texts):\n",
    "  with open(f\"content/cleaned/lecture_{i}.txt\",\"w\") as f:\n",
    "    f.write(text)\n",
    "clean_path = \"content/cleaned\"\n",
    "#sort the files\n",
    "cleaned_text= sorted(\n",
    "    os.listdir(clean_path), key=lambda x: int(x.split(\"_\")\n",
    "    [1].split(\".\")[0]))\n",
    "#cleaned_text\n",
    "\n",
    "cleaned_texts[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45999afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary files saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['summary_1.txt',\n",
       " 'summary_2.txt',\n",
       " 'summary_3.txt',\n",
       " 'summary_4.txt',\n",
       " 'summary_5.txt',\n",
       " 'summary_6.txt',\n",
       " 'summary_7.txt',\n",
       " 'summary_8.txt',\n",
       " 'summary_9.txt',\n",
       " 'summary_10.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the folder for the summarys\n",
    "os.makedirs(\"content/summarys/\",exist_ok=True)\n",
    "for i ,text in enumerate(data[\"summary\"],start=1):\n",
    "  file_path =f\"content/summarys/summary_{i}.txt\"\n",
    "  with open (file_path,\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(str(text))\n",
    "\n",
    "print(\"summary files saved\")\n",
    "\n",
    "#sorting\n",
    "summary=sorted(\n",
    "    os.listdir(\"content/summarys\"),key=lambda X: int(X.split(\"_\")\n",
    "    [1].split(\".\")[0]))\n",
    "summary\n",
    "#os.listdir(\"content/summarys\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a254d313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " {'text': 'last lecture on cybersecurity . cybersecurity protects systems from attacks, malware, exploits etc. concepts: encryption, hashing, firewalls, antivirus, intrusion detection, authentication. threats: phishing , ransomware, ddos, password attacks, mitm attacks. cia triad confidentiality, integrity, availability. best practices: strong passwords, mfa, updates, backups, network monitoring. cybersecurity is super important today be careful online .',\n",
       "  'summary': 'This lecture covers cybersecurity concepts, CIA triad, attack types, best practices, and security mechanisms.'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bulid the list of the Lecture and summery pair,If the pairing is wrong, then the model will learn incorrect mappings.\n",
    "\n",
    "import glob\n",
    "\n",
    "lectures= sorted(glob.glob(\"content/cleaned/*.txt\"))#load all leacture files\n",
    "summaries= sorted(glob.glob(\"content/summarys/*.txt\"))#load all summary files\n",
    "\n",
    "data=[] #store final data set\n",
    "\n",
    "for lec_files, sum_files in zip(lectures, summaries):#zip() joins two lists side-by-side\n",
    "\n",
    "  with open (lec_files, \"r\",encoding = \"utf-8\" )as f: #opens the lecture file,reads all text ,and saves it to lecture_text\n",
    "    lecture_text= f.read()\n",
    "\n",
    "  with open(sum_files,\"r\", encoding=\"utf-8\") as f: #opens the summary file.reads the all text,and saves it to the summsries_text\n",
    "    summary_text = f.read()\n",
    "\n",
    "\n",
    "  data.append({\"text\":lecture_text,\"summary\":summary_text}) #creates the paired entry\n",
    "\n",
    "\n",
    "len(data), data[1] ##how many pairs and first pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed828f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=Dataset.from_list(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "373a90f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= dataset.train_test_split(test_size= 0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691eb34",
   "metadata": {},
   "source": [
    "*TOCKENIZE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bb9b5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Syedh\\anaconda3\\envs\\summarizer\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Syedh\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 62.70 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 122.84 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer #from transformers labiary ,we import the T5tocknzer class\n",
    "\n",
    "tockenizer=T5Tokenizer.from_pretrained(\"t5-small\") # [from_pretrained]This is a method (function inside a class),It loads a pretrained\n",
    "                                                     # tokenizer model[t5_small] from HuggingFace\n",
    "                                                     #our lectures and summerys must be tockenized\n",
    "\n",
    "\n",
    "\n",
    "#/preprocessing or tokenizing each batch\n",
    "#So before training the model, you must:\n",
    "#1:-Convert lecture text ‚Üí token IDs\n",
    "#2:-Convert summary ‚Üí token IDs\n",
    "#3:-Put them in the correct format the model expects\n",
    "#This conversion process is called preprocessing.\n",
    "\n",
    "\n",
    "def preprocess(batch): #batch is the group of multiple samples\n",
    "  #Tokenize the lecture text\n",
    "  model_inputs=tockenizer(batch['text'], #covert the text[batch] into Token IDs\n",
    "                          max_length=512,#Keep the maximum size =512 tockens/words,Not  long text\n",
    "                          truncation=True) #cut if the text is longer than 512\n",
    "\n",
    "  labels=tockenizer(batch[\"summary\"], #covert the summary[batch] into the Token IDs\n",
    "                  \n",
    "                    max_length=150,#with maximun size ,150 tockens , GPT2-small has max_lenght 1024\n",
    "                    truncation=True) #cut if too long\n",
    "\n",
    "                    #{Labels (summary tokens) are used only for:loss calculation (teacher forcing)}\n",
    "                    \n",
    "\n",
    " #model_inputs is result dict created for tokinizing the TEXT BATCH ,\n",
    "\n",
    "                                             #model_inputs = {\n",
    "                                                              #\"input_ids\": [...],       (tokens for lecture text)\n",
    "                                                               # \"attention_mask\": [...] (tells model what to consider as a TOKEN_ID)\n",
    "                                                              #}\n",
    "\n",
    "                                               #For example First Batch has 5 lectures texts , with 300,453,290,500,302 of TOCKEN ID lenght,\n",
    "\n",
    "                                               #A transformer requires all sequences in a batch to have the SAME length \n",
    "\n",
    "                                               #because GPUs compute things in matrix form (equal-sized rows).\n",
    "\n",
    "                                               #So to make the lenght same,we need PADDING ,adding extra zeros to make all sequences equal length.\n",
    "\n",
    "                                               #WHICH means ,300 lecture Tocken IDs will be 512 ,by adding the 212 zeros ,for padding to keep the lenght same(512),\n",
    "\n",
    "                                               #And same goes for other lectures (other tocken ID lenghts)\n",
    "\n",
    "                                               #For model must not pay attention to the padding, So for That (Tokenizer) Creates an [attention_mask](if 1 then consider tocken)\n",
    "                                               #if 0 then ignore \n",
    "\n",
    "                                               #Padding makes the sequences same length.\n",
    "\n",
    "                                               #Attention mask tells the model which part is real and which part is padding.\n",
    "\n",
    "                                               #Together they allow batching.\n",
    "\n",
    "  model_inputs[\"labels\"]= labels[\"input_ids\"] #It adds the summary tokens to the dictionary that the model will receive.\n",
    "                                              #         {\n",
    "                                               #           \"input_ids\": [...],         (lecture tokens)\n",
    "                                               #           \"attention_mask\": [...],\n",
    "                                              #             \"labels\": [...]             (summary tokens)\n",
    "                                                #         }\n",
    "\n",
    "                                                #During training:\n",
    "\n",
    "                                                #The model reads: lecture tokens\n",
    "\n",
    "                                                #The model tries to generate: summary tokens\n",
    "\n",
    "                                               #It compares its output with: labels\n",
    "\n",
    "                                              #Then calculates LOSS\n",
    "\n",
    "                                              #Then improves weights\n",
    "                                              \n",
    "                                              #When you train a model:\n",
    "                                              #input_ids ‚Üí the lecture text (what the model reads)\n",
    "                                              #labels[\"input_ids\"] ‚Üí the summary (the correct output the model should produce)\n",
    "\n",
    "\n",
    "\n",
    "  return model_inputs\n",
    "#apply the function to entire dataset\n",
    "tockenised_dataset =dataset.map(preprocess, batched=True)#HuggingFace Dataset.map() applies your preprocess() function to the entire dataset automatically, \n",
    "                                                          #batch by batch.\n",
    "\n",
    "tockenised_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1cb18e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/8 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:00<00:00, 570.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 179.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tockenised_dataset.save_to_disk(\"content/tokenized_lectures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6225f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
